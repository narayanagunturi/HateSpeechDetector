{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test files\n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"train_E6oV3lV.csv\")\n",
    "test = pd.read_csv(\"test_tweets_anuFYb8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove hashtags, user tags, characters other than numbers, alphabets and replace them with spaces\n",
    "def remove_pattern(input_txt):\n",
    "    input_txt = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",input_txt).split())\n",
    "    return input_txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "#use the above function to tidy up the tweet text for train dataset\n",
    "train['tidy_tweet'] = np.vectorize(remove_pattern)(train['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same for test dataset\n",
    "test['tidy_tweet'] = np.vectorize(remove_pattern)(test['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the entire tweet text to lower for train dataset\n",
    "train[\"tidy_tweet\"] = train[\"tidy_tweet\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the entire tweet text to lower for test dataset\n",
    "test[\"tidy_tweet\"] = test[\"tidy_tweet\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numbers and replace them with empty string for train dataset\n",
    "train[\"tidy_tweet\"] = train[\"tidy_tweet\"].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numbers and replace them with empty string for test dataset\n",
    "test[\"tidy_tweet\"] = test[\"tidy_tweet\"].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"tidy_tweet\"] = train[\"tidy_tweet\"].str.replace(r'\\b\\w{1,2}\\b', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"tidy_tweet\"] = test[\"tidy_tweet\"].str.replace(r'\\b\\w{1,2}\\b', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     when  father  dysfunctional and   selfish  dra...\n",
       "1     thanks for lyft credit  can  use cause they do...\n",
       "2                                   bihday your majesty\n",
       "3                model  love  take with  all the time  \n",
       "4                     factsguide society now motivation\n",
       "5       huge fan fare and big talking before they le...\n",
       "6                                camping tomorrow danny\n",
       "7     the next school year  the year for exams can  ...\n",
       "8      won love the land allin cavs champions clevel...\n",
       "9                                    welcome here      \n",
       "10    ireland consumer price index mom climbed from ...\n",
       "11     are  selfish orlando standwithorlando pulsesh...\n",
       "12                get  see  daddy today days gettingfed\n",
       "13    cnn calls michigan middle school build the wal...\n",
       "14     comment  australia opkillingbay seashepherd h...\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"tidy_tweet\"].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords from test dataset\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "train[\"tidy_tweet\"] = [' '.join(word for word in sentence.split(' ') if word not in stop) for sentence in train[\"tidy_tweet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords from test dataset\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "test[\"tidy_tweet\"] = [' '.join(word for word in sentence.split(' ') if word not in stop) for sentence in test[\"tidy_tweet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_tweet = train[\"tidy_tweet\"].apply(lambda x:x.split(' '))\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "    train['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_tweet = test[\"tidy_tweet\"].apply(lambda x:x.split(' '))\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "    test['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #stem all the words in tweet test for train dataset\n",
    "# tokenized_tweet = test[\"tidy_tweet\"].apply(lambda x:x.split(' '))\n",
    "# tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "# for i in range(len(tokenized_tweet)):\n",
    "#     tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "# test['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use k (7) fold cross validation\n",
    "k_fold = KFold(n_splits=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold.get_n_splits(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(use_idf=True, smooth_idf=True, ngram_range=(1,2), max_df = 0.5, min_df=1,  lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Use a pipeline for tfidf, naive bayes\n",
    "# pipeline = Pipeline([\n",
    "#     ('Vect', CountVectorizer(stop_words='english', lowercase=True,ngram_range=(1,2))),\n",
    "#     ('tfidf', TfidfTransformer(use_idf=True, smooth_idf=True)),\n",
    "#     ('clf', MultinomialNB(alpha=0.01))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = np.array([[0,0],[0,0]])\n",
    "scores = []\n",
    "#split the train data into train data and validation data and fit the train data into pipeline and predict the validation set usig pipeline\n",
    "for train_indices, test_indices in k_fold.split(train):\n",
    "    train_text = train.iloc[train_indices]['tidy_tweet'].values\n",
    "    train_y = train.iloc[train_indices]['label'].values.astype(str)\n",
    "    \n",
    "    test_text = train.iloc[test_indices]['tidy_tweet'].values\n",
    "    test_y = train.iloc[test_indices]['label'].values.astype(str)\n",
    "    train_x = tfidf.fit_transform(train_text)\n",
    "    nb.fit(train_x, train_y)\n",
    "#     pipeline.fit(train_text, train_y)\n",
    "    test_x = tfidf.transform(test_text)\n",
    "    predictions = nb.predict(test_x)\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label='0')\n",
    "    scores.append(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x21 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 24 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit_transform([\"hello how are you\", \"hello i am fine\", \"no i am not fine\",\"that is ok\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.transform([\"fine fine\"], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.9824226409544088\n",
      "confusion_matrix\n",
      "[[29511   209]\n",
      " [  847  1395]]\n"
     ]
    }
   ],
   "source": [
    "#print f1score and confusion matrix\n",
    "print(\"score\", (sum(scores)/len(scores)))\n",
    "print(\"confusion_matrix\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions for test dataset\n",
    "predictions2 = pipeline.predict(test['tidy_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test[['id', 'label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.to_csv('test_predictions.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pipeline_pickle = open('E:/pipeline.pickle', 'wb')\n",
    "pickle.dump(pipeline, pipeline_pickle)\n",
    "pipeline_pickle.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
